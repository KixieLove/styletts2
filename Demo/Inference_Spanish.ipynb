{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1fa54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed to directory: d:\\\n",
      "Device: cuda\n",
      "Current working directory: d:\\\n"
     ]
    }
   ],
   "source": [
    "# 0) Set up paths - use absolute path to repo root\n",
    "import os\n",
    "repo_root = r'd:\\Tesis\\StyleTTS2'\n",
    "os.chdir(repo_root)\n",
    "print('Changed to directory:', os.getcwd())\n",
    "\n",
    "# 1) Imports and deterministic setup\n",
    "import time\n",
    "import random\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# deterministic seeds for reproducibility (optional)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "print('Current working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b486796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) helper functions and audio params\n",
    "sr = 24000\n",
    "mean, std = -4, 4\n",
    "\n",
    "# mel transform used across repo\n",
    "to_mel = torchaudio.transforms.MelSpectrogram(n_mels=80, n_fft=2048, win_length=1200, hop_length=300)\n",
    "\n",
    "def length_to_mask(lengths):\n",
    "    mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)\n",
    "    mask = torch.gt(mask+1, lengths.unsqueeze(1))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def preprocess(wave):\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b47a96",
   "metadata": {},
   "source": [
    "## 3) Load Config and Set Checkpoint\n",
    "\n",
    "By default this notebook loads `Configs/config.es.yml`. The `checkpoint_path` defaults to `os.path.join(config['log_dir'], 'epoch_2nd_00004.pth')` — change this to the checkpoint you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ccabb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config: d:\\Tesis\\StyleTTS2\\Configs\\config.es.yml\n",
      "Config file exists: True\n",
      "Default checkpoint: logs/angelina_es\\epoch_2nd_00004.pth\n"
     ]
    }
   ],
   "source": [
    "cfg_path = r'd:\\Tesis\\StyleTTS2\\Configs\\config.es.yml'\n",
    "config = yaml.safe_load(open(cfg_path))\n",
    "print('Loaded config:', cfg_path)\n",
    "print('Config file exists:', os.path.exists(cfg_path))\n",
    "\n",
    "# default checkpoint (change if needed)\n",
    "default_ckpt = os.path.join(config['log_dir'], 'epoch_2nd_%05d.pth' % 4)\n",
    "checkpoint_path = default_ckpt\n",
    "print('Default checkpoint:', checkpoint_path)\n",
    "\n",
    "# If your training log_dir uses an absolute path, update root_path in config or set checkpoint_path manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46974df2",
   "metadata": {},
   "source": [
    "## 4) Load model components (ASR, F0, PLBERT) and build StyleTTS2 model\n",
    "\n",
    "This replicates the same setup used during training so inference uses identical components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa9436bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# import model builders & helpers from repo\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_model, load_checkpoint, load_F0_models, load_ASR_models\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mUtils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPLBERT\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_plbert\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtext_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextCleaner\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "# import model builders & helpers from repo\n",
    "from models import build_model, load_checkpoint, load_F0_models, load_ASR_models\n",
    "from Utils.PLBERT.util import load_plbert\n",
    "from text_utils import TextCleaner\n",
    "from utils import recursive_munch\n",
    "\n",
    "# Load ASR, F0 extractor and PL-BERT according to config\n",
    "ASR_config = config.get('ASR_config', None)\n",
    "ASR_path = config.get('ASR_path', None)\n",
    "print('ASR config:', ASR_config, 'ASR path:', ASR_path)\n",
    "\n",
    "if ASR_config and ASR_path:\n",
    "    text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
    "else:\n",
    "    text_aligner = None\n",
    "\n",
    "F0_path = config.get('F0_path', None)\n",
    "if F0_path:\n",
    "    pitch_extractor = load_F0_models(F0_path)\n",
    "else:\n",
    "    pitch_extractor = None\n",
    "\n",
    "plbert_dir = config.get('PLBERT_dir', None)\n",
    "plbert = load_plbert(plbert_dir) if plbert_dir else None\n",
    "\n",
    "# Build model\n",
    "model_params = recursive_munch(config['model_params'])\n",
    "model = build_model(model_params, text_aligner, pitch_extractor, plbert)\n",
    "_ = [model[key].to(device) for key in model]\n",
    "_ = [model[key].eval() for key in model]\n",
    "print('Model built and moved to device')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e5bb82",
   "metadata": {},
   "source": [
    "## 5) Load checkpoint weights\n",
    "\n",
    "If a checkpoint uses a `module.` prefix (from DataParallel), the loader will handle that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9815d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    print('Checkpoint not found:', checkpoint_path)\n",
    "    print('List files in', config['log_dir'], 'to choose a different checkpoint:')\n",
    "    try:\n",
    "        print('\\n'.join(sorted(os.listdir(config['log_dir']))))\n",
    "    except Exception as e:\n",
    "        print('Could not list directory:', e)\n",
    "else:\n",
    "    print('Loading checkpoint:', checkpoint_path)\n",
    "    params_whole = torch.load(checkpoint_path, map_location='cpu')\n",
    "    params = params_whole.get('net', params_whole)\n",
    "\n",
    "    # try loading directly, otherwise handle DataParallel `module.` prefix\n",
    "    for key in model:\n",
    "        if key in params:\n",
    "            try:\n",
    "                model[key].load_state_dict(params[key])\n",
    "                print(f'{key} loaded')\n",
    "            except Exception:\n",
    "                # try removing `module.` prefix\n",
    "                sd = params[key]\n",
    "                new_sd = OrderedDict()\n",
    "                for k, v in sd.items():\n",
    "                    name = k[7:] if k.startswith('module.') else k\n",
    "                    new_sd[name] = v\n",
    "                model[key].load_state_dict(new_sd, strict=False)\n",
    "                print(f'{key} loaded with module.* prefix removal')\n",
    "\n",
    "    print('Checkpoint loaded (weights).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e83a2b",
   "metadata": {},
   "source": [
    "## 6) Build Diffusion Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c496c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.diffusion.sampler import DiffusionSampler, ADPM2Sampler, KarrasSchedule\n",
    "\n",
    "sampler = DiffusionSampler(\n",
    "    model.diffusion.diffusion,\n",
    "    sampler=ADPM2Sampler(),\n",
    "    sigma_schedule=KarrasSchedule(sigma_min=0.0001, sigma_max=3.0, rho=9.0),\n",
    "    clamp=False\n",
    ")\n",
    "print('Sampler ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcbddd9",
   "metadata": {},
   "source": [
    "## 7) Prepare phonemizer (Spanish) and TextCleaner\n",
    "\n",
    "We use eSpeak phonemizer for Spanish. Ensure `phonemizer` package is installed in your environment. If not available, you can feed raw characters instead (less accurate prosody)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d3086",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import phonemizer\n",
    "    from phonemizer.backend import EspeakBackend\n",
    "    global_phonemizer = EspeakBackend(language='es', preserve_punctuation=True, with_stress=True)\n",
    "    print('Phonemizer loaded (es)')\n",
    "except Exception as e:\n",
    "    print('Phonemizer not available:', e)\n",
    "    global_phonemizer = None\n",
    "\n",
    "textcleaner = TextCleaner()\n",
    "\n",
    "# NLTK tokenizer will be used for tokenization\n",
    "import nltk\n",
    "\n",
    "print('Text utilities ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5d2856",
   "metadata": {},
   "source": [
    "## 8) Inference function\n",
    "\n",
    "This function mirrors the LJSpeech demo: it tokenizes / phonemizes Spanish text, gets BERT embedding, samples a style vector via the diffusion sampler, predicts durations and prosody, then uses the decoder to synthesize waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09525bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "def inference(text, noise=None, diffusion_steps=5, embedding_scale=1.0):\n",
    "    text = text.strip()\n",
    "    text = text.replace('\"', '')\n",
    "\n",
    "    if global_phonemizer is not None:\n",
    "        ps = global_phonemizer.phonemize([text])\n",
    "        ps = word_tokenize(ps[0])\n",
    "        ps = ' '.join(ps)\n",
    "    else:\n",
    "        # fallback: basic whitespace tokenizer\n",
    "        ps = ' '.join(word_tokenize(text))\n",
    "\n",
    "    tokens = textcleaner(ps)\n",
    "    tokens.insert(0, 0)\n",
    "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(tokens.device)\n",
    "        text_mask = length_to_mask(input_lengths).to(tokens.device)\n",
    "\n",
    "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
    "        bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
    "        d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
    "\n",
    "        if noise is None:\n",
    "            noise = torch.randn(1,1,256).to(device)\n",
    "\n",
    "        s_pred = sampler(noise,\n",
    "                         embedding=bert_dur[0].unsqueeze(0),\n",
    "                         num_steps=diffusion_steps,\n",
    "                         embedding_scale=embedding_scale).squeeze(0)\n",
    "\n",
    "        s = s_pred[:, 128:]\n",
    "        ref = s_pred[:, :128]\n",
    "\n",
    "        d = model.predictor.text_encoder(d_en, s, input_lengths, text_mask)\n",
    "\n",
    "        x, _ = model.predictor.lstm(d)\n",
    "        duration = model.predictor.duration_proj(x)\n",
    "        duration = torch.sigmoid(duration).sum(axis=-1)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "\n",
    "        pred_dur[-1] += 5\n",
    "\n",
    "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            pred_aln_trg[i, c_frame:c_frame + int(pred_dur[i].data)] = 1\n",
    "            c_frame += int(pred_dur[i].data)\n",
    "\n",
    "        en = (d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device))\n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "        out = model.decoder((t_en @ pred_aln_trg.unsqueeze(0).to(device)),\n",
    "                            F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
    "\n",
    "    return out.squeeze().cpu().numpy()\n",
    "\n",
    "print('Inference function ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c0bdc",
   "metadata": {},
   "source": [
    "## 9) Example: synthesize a Spanish sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a5ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Spanish text (edit as you like)\n",
    "spanish_text = \"Hola, esta es una prueba rápida del modelo StyleTTS2 entrenado con datos en español.\"\n",
    "\n",
    "# generate noise and run inference\n",
    "noise = torch.randn(1,1,256).to(device)\n",
    "start = time.time()\n",
    "wav = inference(spanish_text, noise=noise, diffusion_steps=5, embedding_scale=1.0)\n",
    "print('Synthesis time:', time.time() - start, 'seconds')\n",
    "\n",
    "# display audio widget\n",
    "display(ipd.Audio(wav, rate=sr))\n",
    "\n",
    "# save output to disk\n",
    "out_path = 'Demo/output_spanish.wav'\n",
    "sf.write(out_path, wav, sr)\n",
    "print('Saved:', out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1332761b",
   "metadata": {},
   "source": [
    "## 10) Optional: longer generation or different styles\n",
    "\n",
    "You can increase `diffusion_steps` for more diverse / higher-quality samples (slower), or change `embedding_scale` to control expressiveness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "styletts2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
